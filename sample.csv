decision	metareview	review1	R1	C1	hedge_score1	sentiment1	review2	R2	C2	hedge_score2	sentiment2	review3	R3	C3	hedge_score3	sentiment3	year
1	The reviewers agreed that the work addresses an important problem. There was disagreement as to the correctness of the arguments in the paper: one of these reviewers was eventually convinced. The other pointed out another two issue in their final post, but it seems that 1. the first is easily adopted and does not affect the correctness of the experiments and 2. the second was fixed in the second revision. Ideally these would be rechecked by the third reviewer, but ultimately the correctness of the work is the authors responsibility. Some related work (by McAllister) was pointed out late in the process. I encourage the authors to take this related work seriously in any revisions. It deserves more than two sentences.	==Main comments The authors connect dropout parameters to a bound of the Rademacher complexity (Rad) of the network. While it is great to see deep learning techniques inspired by learning theory, I think the paper makes too many leaps and the Rad story is ultimately unconvincing. Perhaps it is better to start with the resulting regularizer, and the interesting direct optimization of dropout parameters. In its current form, the following leaps problematic and were not addressed in the paper: 1) Why is is adding Rad as a regularizer reasonable? Rad is usually hard to compute, and most useful for bounding the generalization error. It would be interesting if it also turns out to be a good regularizer, but the authors do not say why nor cite anything. Like the VC dimension, Rad itself depends on the model class, and cannot be directly optimized. Even if you can somehow optimize over the model class, these quantities give very loose bounds, and do not equal to generalization error. For example, I feel even just adding the actual generalization error bound is more natural. Would it make sense to just add Rad to the objective in this way for a linear model? 2) Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad? The actual resulting regularizer turns out to be a weight penalty but this seems to be a rather loose bound that might not have too much to do with Rad anymore. There should be some analysis on how loose this bound is, and if this looseness matter at all. The empirical results themselves seem reasonable, but the results are not actually better than simpler methods in the corresponding tasks, the interpretation is less confident. Afterall, it seems that the proposed method had several parameters that were turned, where the analogous parameters are not present in the competing methods. And the per unit dropout rates are themselves additional parameters, but are they actually good use of parameters? ==Minor comments The optimization is perhaps also not quite right, since this requires taking the gradient of the dropout parameter in the original objective. While the authors point out that one can use the mean, but that is more problematic for the gradient than for normal forward predictions. The gradient used for regular learning is not based on the mean prediction, but rather the samples. tiny columns surrounding figures are ugly and hard to read 	3	3	0.0390243902439024	-0.7357	An important contribution. The paper is well written. Some questions that needs to be better answered are listed here. 1. The theorem is difficult to decipher. Some remarks needs to be included explaining the terms on the right and what they mean with respect to learnability or complexity. 2. How does the regularization term in eq (2) relate to the existing (currently used) norm based regularizers in deep network learning? It may be straight forward but some small simulation/plots explaining this is important. 3. Apart from the accuracy results, the change in computational time for working with eq (2), rather than using existing state-of-the-art deep network optimization needs to be reported? How does this change vary with respect to dataset and network size (beyond the description of scaled regularization in section 4)? 4. Confidence intervals needs to be computed for the retain-rates (reported as a function of epoch). This is critical both to evaluate the stability of regularizers as well as whether the bound from theorem is strong. 5. Did the evaluations show some patterns on the retain rates across different layers? It seems from Figure 3,4 that retain rates in lower layers are more closer to 1 and they decrease to 0.5 as depth increases. Is this a general pattern? 6. It has been long known that dropout relates to non-negative weighted averaging of partially learned neural networks and dropout rate of 0.5 provides best dymanics. The evaluations say that clearly 0.5 for all units/layers us not correct. What does this mean in terms of network architecture? Is it that some layers are easy to average (nothing is learned there, so dropped networks have small variance), while some other layers are sensitive? 7. What are some simple guidelines for choosing the values of p and q? Again it appears p=q=2 is the best, but need confidence intervals here to say anything substantial. 	3	5	0.0222929936305732	0.9947	"This paper studies the adjustment of dropout rates which is a useful tool to prevent the overfitting of deep neural networks. The authors derive a generalization error bound in terms of dropout rates. Based on this, the authors propose a regularization framework to adaptively select dropout rates. Experimental results are also given to verify the theory. Major comments: (1) The Empirical Rademacher complexity is not defined. For completeness, it would be better to define it at least in the appendix. (2) I can not follow the inequality (5). Especially, according to the main text, f^L is a vector-valued function . Therefore, it is not clear to me the meaning of sumsigma_if^L(x_i,w) in (5). (3) I can also not see clearly the third equality in (9). Note that f^l is a vector-valued function. It is not clear to me how it is related to a summation over j there. (4) There is a linear dependency on the number of classes in Theorem 3.1. Is it possible to further improve this dependency? Minor comments: (1) Section 4: 1e-3,1e-4,1e-5 is not consistent with 1e^{-3}, 1e^{-4},1e^{-5} (2) Abstract: there should be a space before ""Experiments"". (3) It would be better to give more details (e.g., page, section) in citing a book in the proof of Theorem 3.1 Summary: The mathematical analysis in the present version is not rigorous. The authors should improve the mathematical analysis. ---------------------------- After Rebuttal: Thank you for revising the paper. I think there are still some possible problems. Let us consider eq (12) in the appendix on the contraction property of Rademacher complexity (RC). (1) Since you consider a variant of RC with absolute value inside the supermum, to my best knowledge, the contraction property (12) should involve an additional factor of 2, see, e.g., Theorem 12 of ""Rademacher and Gaussian Complexities: Risk Bounds and Structural Results"" by Bartlett and Mendelson. Since you need to apply this contraction property L times, there should be a factor of 2^L in the error bound. This make the bound not appealing for neural networks with a moderate L. (2) Second, the function g involves an expectation w.r.t. r before the activation function. I am not sure whether this existence of expectation w.r.t. r would make the contraction property applicable in this case."	3	3	0.0527704485488126	0.8412	2018
1	The reviewers agree that the problem of learning learning credit assignment from terminal rewards is interesting, and that the presented approach is promising. There are some concerns regarding the rigor and correctness of the theoretical results, and I ask the authors to improve those aspects of the paper. I also ask the authors to the result figures easier to read. The chosen colors are not ideal and the use of log-scale x-axis is not standard. Finally, including DAgger in the same plot is confusing assuming that DAgger user more information.	"The authors propose a new episodic reinforcement learning algorithm based on contextual bandit oracles. The key specificity of this algorithm is its ability to deal with the credit assignment problem by learning automatically a progressive ""reward shaping"" (the residual losses) from a feedback that is only provided at the end of the epochs. The paper is dense but well written. The theoretical grounding is a bit thin or hard to follow. The authors provide a few regret theoretical results (that I did not check deeply) obtained by reduction to ""value-aware"" contextual bandits. The experimental section is solid. The method is evaluated on several RL environments against state of the art RL algorithms. It is also evaluated on bandit structured prediction tasks. An interesting synthetic experiment (Figure 4) is also proposed to study the ability of the algorithm to work on both decomposable and non-decomposable structured prediction tasks. Question 1: The credit assignment approach you propose seems way more sophisticated than eligibility traces in TD learning. But sometimes old and simple methods are not that bad. Could you develop a bit on the relation between RESLOPE and eligibility traces ? Question 2: RESLOPE is built upon contextual bandits which require a stationary environment. Does RESLOPE inherit from this assumption? Typos: page 1 ""scalar loss that output."" -> ""scalar loss."" "", effectively a representation"" -> "". By effective we mean effective in term of credit assignment."" page 5 ""and MTR"" -> ""and DR"" page 6 ""in simultaneously."" -> ??? "".In greedy"" -> "". In greedy"" "	3	2	0.0157480314960629	0.9847	"After reading the other reviews and the authors responses, I am satisfied that this paper is above the accept threshold. I think there are many areas of further discussion that the authors can flesh out (as mentioned below and in other reviews), but overall the contribution seems solid. I also appreciate the reviewers efforts to run more experiments and flesh out the discussion in the revised version of the submission. Final concluding thoughts: -- Perhaps pi^ref was somehow better for the structured prediction problems than RL problems? -- Can one show regret bound for multi-deviation if one doesnt have to learn x (i.e., we were given a good x a priori)? --------------------------------------------- ORIGINAL REVIEW First off, I think this paper is potentially above the accept threshold. The ideas presented are interesting and the results are potentially interesting as well. However, I have some reservations, a significant portion of which stem from not understanding aspects of the proposed approach and theoretical results, as outlined below. The algorithm design and theoretical results in the appendix could be made substantially more rigorous. Specifically: -- basic notations such as regret (in Theorem 1), the total reward (J), Q-value (Q), and value function (V) are not defined. While these concepts are fairly standard, it would be highly beneficial to define them formally. -- Im not convinced that the ""terms in the parentheses"" (Eq. 7) are ""exactly the contextual bandit cost"". I would like to see a more rigorous derivation of the connection. For instance, one could imagine that the policy disadvantage should be the difference between the residual costs of the bandit algorithm and the reference policy, rather than just the residual cost of the bandit algorithm. -- Im a little unclear in the proof of Theorem 1 where Q(s,pi_n) from Eq 7 fits into Eq 8. -- The residual cost used for CB.update depends on estimated costs at other time steps h!=h_dev. Presumably, these estimated costs will change as learning progresses. How does one reconcile that? I imagine that it could somehow work out using a bandit algorithm with adversarial guarantees, but I can also imagine it not working out. I would like to see a rigorous treatment of this issue. -- It would be nice to see an end-to-end result that instantiates Theorem 1 (and/or Theorem 2) with a contextual bandit algorithm to see a fully instantiated guarantee. With regards to the algorithm design itself, I have some confusions: -- How does one create x in practice? I believe this is described in Appendix H, but its not obvious. -- What happens if we dont have a good way to generate x and it must be learned as well? Id imagine one would need larger RNNs in that case. -- If x is actually learned on-the-fly, how does that impact the theoretical results? -- I find it curious that theres no notion of future reward learning in the learning algorithm. For instance, in Q learning, one directly models the the long-term (discounted) rewards during learning. In fact, the theoretical analysis talks about advantage functions as well. It would be nice to comment on this aspect at an intuitive level. With regards to the experiments: -- I find it very curious that the results are so negative for using only 1-dev compared to multi-dev (Figure 9 in Appendix). Given that much of the paper is devoted to 1-dev, its a bit disappointing that this issue is not analyzed in more detail, and furthermore the results are mostly hidden in the appendix. -- Its not clear if a reference policy was used in the experiments and what value of beta was used. -- Can the authors speculate about the difference in performance between the RL and bandit structured prediction settings? My personal conjecture is that the bandit structured prediction settings are more easily decomposable additively, which leads to a greater advantage of the proposed approach, but I would like to hear the authors thoughts. Finally, the overall presentation of this paper could be substantially improved. In addition to the above uncertainties, some more points are described below. I dont view these points as ""deal breakers"" for determining accept/reject. -- This paper uses too many examples, from part-of-speech tagging to credit assignment in determining paths. I recommend sticking to one running example, which substantially reduces context switching for the reader. In every such example, there are extraneous details are not relevant to making the point, and the reader needs to spend considerable effort figuring that out for each example used. -- Inconsistent language. For instance, x is sometimes referred to as the ""input example"", ""context"" and ""features"". -- At the end of page 4, ""Internally, ReslopePolicy takes a standard learning to search step."" Two issues: 1) ReslopePolicy is not defined or referred to anywhere else. 2) is the remainder of that paragraph a description of a ""standard learning to search step""? -- As mentioned before, Regret is not defined in Theorem 1 & 2. -- The discussion of the high-level algorithmic concepts is a bit diffuse or lacking. For instance, one key idea in the algorithmic development is that its sufficient to make a uniformly random deviation. Is this idea from the learning to search literature? If so, it would be nice to highlight this in Section 2.2. "	3	5	0.0317100792751981	0.9968	The authors present a new RL algorithm for sparse reward tasks. The work is fairly novel in its approach, combining a learned reward estimator with a contextual bandit algorithm for exploration/exploitation. The paper was mostly clear in its exposition, however some additional information of the motivation for why the said reduction is better than simpler alternatives would help. Pros 1. The results on bandit structured prediction problems are pretty good 2. The idea of a learnt credit assignment function, and using that to separate credit assignment from the exploration/exploitation tradeoff is good. Cons: 1. The method seems fairly more complicated than PPO / A2C, yet those methods seem to perform equally well on the RL problems (Figure 2.). It also seems to be designed only for discrete action spaces. 2. Reslope Boltzmann performs much worse than Reslope Bootstrap, thus having a bag of policies helps. However, in the comparison in Figures 2 and 3, the policy gradient methods dont have the advantage of using a bag of policies. A fairer comparison would be to compare with methods that use ensembles of Q-functions. (like this https://arxiv.org/abs/1706.01502 by Chen et al.). The Q learning methods in general would also have better sample efficiency than the policy gradient methods. 3. The method claims to learn an internal representation of a denser reward function for the sparse reward problem, however the experimental analysis of this is pretty limited (Section 5.3). It would be useful to do a more thorough investigation of whether it learnt a good credit assignment function in the games. One way to do this would be to check the qualitative aspects of the function in a well understood game, like Blackjack. Suggestions: 1. What is the advantage of the method over a simple RL method that predicts a reward at every step (such that the dense rewards add up to match the sparse reward for the episode), and uses this predicted dense reward to perform RL? This, and also a bigger discussion on prior bandit learning methods like LOLS will help under the context for why we’re performing the reduction stated in the paper. Significance: While the method is novel and interesting, the experimental analysis and the explanations in the paper leave it unclear as to whether its significant compared to prior work. Revision: I thank the authors for addressing some of my concerns. The comparison with relative gain of bootstrap wrt ensemble of policies still needs more thorough experimentation, but the approach is novel and as the authors point out, does improve continually with better Contextual Bandit algorithms. I update my review to 6. 	3	4	0.0160919540229885	0.9941	2018
1	Thank you for submitting you paper to ICLR. This paper was enhanced noticeably in the rebuttal period and two of the reviewers improved their score as a result. There is a good range of experimental work on a number of different tasks. The addition of the comparison with Liu & Feng, 2016 to the appendix was sensible. Please make sure that the general conclusions drawn from this are explained in the main text and also the differences to Tran et al., 2017 (i.e. that the original model can also be implicit in this case).	Thank you for the feedback, and I think many of my concerns have been addressed. I think the paper should be accepted. ==== original review ==== Thank you for an interesting read. Approximate inference with implicit distribution has been a recent focus of the research since late 2016. I have seen several papers simultaneously proposing the density ratio estimation idea using GAN approach. This paper, although still doing density ratio estimation, uses kernel estimators instead and thus avoids the usage of discriminators. Furthermore, the paper proposed a new type of implicit posterior approximation which uses intuitions from matrix factorisation. I do think that another big challenge that we need to address is the construction of good implicit approximations, which is not well studied in previous literature (although this is a very new topic). This paper provides a good start in this direction. However several points need to be clarified and improved: 1. There are other ways to do implicit posterior inference such as amortising deterministic/stochastic dynamics, and approximating the gradient updates of VI. Please check the literature. 2. For kernel based density ratio estimation methods, you probably need to cite a bunch of Sugiyama papers besides (Kanamori et al. 2009). 3. Why do you need to introduce both regression under p and q (the reverse ratio trick)? I didnt see if you have comparisons between the two. From my perspective the reverse ratio trick version is naturally more suitable to VI. 4. Do you have any speed and numerical issues on differentiating through alpha (which requires differentiating K^{-1})? 5. For kernel methods, kernel parameters and lambda are key to performances. How did you tune them? 6. For the celebA part, can you compute some quantitative metric, e.g inception score? 	3	4	0.0241379310344827	0.968	"This paper presents Kernel Implicit Variational Inference (KIVI), a novel class of implicit variational distributions. KIVI relies on a kernel approximation to directly estimate the density ratio. Importantly, the optimal kernel approximation in KIVI has closed-form solution, which allows for faster training since it avoids gradient ascent steps that may soon get ""outdated"" as the optimization over the variational distribution runs. The paper presents experiments on a variety of scenarios to show the performance of KIVI. Up to my knowledge, the idea of estimating the density ratio using kernels is novel. I found it interesting, specially since there is a closed-form solution for this estimate. The closed form solution involves a matrix inversion, but this shouldnt be an issue, as the matrix size is controlled by the number of samples, which is a parameter that the practitioner can choose. I also found interesting the implicit MMNN architecture proposed in Section 4. The experiments seem convincing too, although I believe the paper could probably be improved by comparing with other implicit VI methods, such as [Liu & Feng], [Tran et al.], or others. My major criticism with the paper is the quality of the writing. I found quite a few errors in every page, which significantly affects readability. I strongly encourage the authors to carefully review the entire paper and search for typos, grammatical errors, unclear sentences, etc. Please find below some further comments broken down by section. Section 1: In the introduction, it is unclear to me what ""protect these models"" means. Also, in the second paragraph, the authors talk about ""often leads to biased inference"". The concept to ""biased inference"" is unclear. Finally, the sentence ""the variational posterior we get in this way does not admit a tractable likelihood"" makes no sense to me; how can a posterior admit (or not admit) a likelihood? Section 3: The first paragraph of the KIVI section is also unclear to me. In Section 3.1, it looks like the cost function L(hat(r)) is different from the loss in Eq. 1, so it should have a different notation. In Eq. 4, I found it confusing whether L(r)=J(r). Also, it would be nice to include a brief description of why the expectation in Eq. 4 is taken w.r.t. p(z) instead of q(z), for those readers who are less familiar with [Kanamori et al.]. Finally, the motivation behind the ""reverse ratio trick"" was unclear to me (the trick is clear, but I didnt fully understand why its needed). Section 4: The first paragraph of the example can be improved with a brief discussion of why the methods of [Mescheder et al.] and [Song et al.] ""are nor applicable"". Also, the paragraph above Eq. 11 (""When modeling a matrix..."") was unclear to me. Section 6: In Figure 1(a), I think there must be something wrong, because it is well-known that VI tends to cover one of the modes of the posterior only due to the form of the KL divergence (in contrast to EP, which should look like the curve in the figure). Additionally, Figure 3(a) (and the explanation in the text) was unclear to me. Finally, I disagree with the discussion regarding overfitting in Figure 3(b): that plot doesnt show overfitting because it is a plot of the training loss (and overfitting occurs on test); instead it looks like an optimization issue that makes the bound decrease. **** EDITS AFTER AUTHORS REBUTTAL **** I increased the rating to 7 after reading the revised version. "	3	3	0.0328151986183074	0.8732	Update: I read the other reviews and the authors rebuttal. Thanks to the authors for clarifying some details. Im still against the paper being accepted. But I dont have a strong opinion and will not argue against so if other reviewers are willing. ------ The authors propose Kernel Implicit VI, an algorithm allowing implicit distributions as the posterior approximation by employing kernel ridge regression to estimate a density ratio. Unlike current approaches with adversarial training, the authors argue this avoids the problems of noisy ratio estimation, as well as potentially high-dimensional inputs to the discriminator. The work has interesting ideas. Unfortunately, Im not convinced that the method overcomes these difficulties as they argue in Sec 3.2. An obvious difficulty with kernel ridge regression in practice is that its complete inaccuracy to estimate high-dimensional density ratios. This is especially the case given a limited number of samples from both p and q (which is the same problem as previous methods) as well as the RBF kernel. While the RBF kernel still takes the same high-dimensional inputs and does not involve learning massive sets of parameters, it also does not scale well at all for accurate estimation. This is the same problem as related approaches with Stein variational gradient descent; namely, it avoids minimax problems as in adversarial training by implicitly integrating over the discriminator function space using the kernel trick. This flaw has rather deep implications. For example, my understanding of the implicit VI on the Bayesian neural network in Sec 4 is that it ends up as cross-entropy minimization subject to a poorly estimated KL regularizer. Id like to see just how much entropy the implicit approximation has instead of concnetrating toward a point; or more directly, what the implicit posterior approximation looks like compared to a true posterior inferred by, say, HMC as the ground truth. This approach also faces difficulties that the naive Gaussian approximation applied to Bayesian neural nets does not: implicit approximations cannot exploit the local reparameterization trick and are therefore limited to specific architectures that does not involve sampling very large weight matrices. The authors report variational lower bounds, which Im not sure is really a lower bound. Namely, the bias incurred by the ratio estimation makes it difficult to compare numbers. An obvious but very illustrative experiment Id like to see would be the accuracy of the KL estimator on problems where we can compute it tractably, or where we can Monte Carlo estimate it very well under complicated but tractable densities. I also suggest the authors perform the experiment suggested above with HMC as ground truth on a non-toy problem such as a fairly large Bayesian neural net.	2	4	0.0202247191011235	-0.7804	2018
0	The paper provides a constrained mutual information objective function whose Lagrangian dual covers several existing generative models. However reviewers are not convinced of the significance or usefulness of the proposed unifying framework (at least from the way results are presented currently in the paper). Authors have not taken any steps towards revising the paper to address these concerns. Improving the presentation to bring out the significance/utility of the proposed unifying framework is needed.	"Update after rebuttal ========== Thanks for your response on my questions. The stated usefulness of the method unfortunately do not answer my worry about the significance. It remains unclear to me how much ""real"" difference the presented results would make to advance the existing work on generative models. Also, the authors did not promised any major changes in the final version in this direction, which is why I have reduced my score. I do believe that this work could be useful and should be resubmitted. There are two main things to improve. First, the paper need more work on improving the clarity. Second, more work needs to be added to show that the paper will make a real difference to advance/improve existing methods. ========== Before rebuttal ========== This paper proposes an optimization problem whose Lagrangian duals contain many existing objective functions for generative models. Using this framework, the paper tries to generalize the optimization problems by defining computationally-tractable family which can be expressed in terms of existing objective functions. The paper has interesting elements and the results are original. The main issue is that the significance is unclear. The writing in Section 3 is unclear for me, which further made it challenging to understand the consequences of the theorems presented in that section. Here is a big-picture question that I would like to know answer for. Do the results of sec 3 help us identify a more useful/computationally tractable model than exiting approaches? Clarification on this will help me evaluate the significance of the paper. I have three main clarification points. First, what is the importance of T1, T2, and T3 classes defined in Def. 7, i.e., why are these classes useful in solving some problems? Second, is the opposite relationship in Theorem 1, 2, and 3 true as well, e.g., is every linear combination of beta-ELBO and VMI is equivalent to a likelihood-based computable-objective of KL info-encoding family? Is the same true for other theorems? Third, the objective of section 3 is to show that ""only some choices of lambda lead to a dual with a tractable equivalent form"". Could you rewrite the theorems so that they truly reflect this, rather than stating something which only indirectly imply the main claim of the paper. Some small comments: - Eq. 4. It might help to define MI to remind readers. - After Eq. 7, please add a proof (may be in the Appendix). It is not that straightforward to see this. Also, I suppose you are saying Eq. 3 but with f from Eq. 4. - Line after Eq. 8, D_i is ""one"" of the following... Is it always the same D_i for all i or it could be different? Make this more clear to avoid confusion. - Last line in Para after Eq. 15, ""This neutrality corresponds to the observations made in.."" It might be useful to add a line explaining that particular ""observation"" - Def. 7, the names did not make much sense to me. You can add a line explaining why this name is chosen. - Def. 8, the last equation is unclear. Does the first equivalence impy the next one? - Writing in Sec. 3.3 can be improved. e.g., ""all linear operations on log prob."" is very unclear, ""stated computational constraints"" which constraints? "	2	4	0.0255474452554744	0.9785	"EDIT: I have read the authors rebuttals and other reviews. My opinion has not been changed. I recommend the authors significantly revise their work, streamlining the narrative and making clear what problems and solutions they solve. While I enjoy the perspective of unifying various paths, its unclear what insights come from a simple reorganization. For example, what new objectives come out? Or given this abstraction, what new perspectives or analysis is offered? --- The authors propose an objective whose Lagrangian dual admits a variety of modern objectives from variational auto-encoders and generative adversarial networks. They describe tradeoffs between flexibility and computation in this objective leading to different approaches. Unfortunately, Im not sure what specific contributions come out, and the paper seems to meander in derivations and remarks that I didnt understand what the point was. First, its not clear what this proposed generalization offers. Its a very nuanced and not insightful construction (eq. 3) and with a specific choice of a weighted sum of mutual informations subject to a combinatorial number of divergence measure constraints, each possibly held in expectation (eq. 5) to satisfy the chosen subclass of VAEs and GANs; and with or without likelihoods (eq. 7). What specific insights come from this that isnt possible without the proposed generalization? Its also not clear with many GAN algorithms that reasoning with their divergence measure in the limit of infinite capacity discriminators is even meaningful (e.g., Arora et al., 2017; Fedus et al., 2017). Its only true for consistent objectives such as MMD-GANs. Section 4 seems most pointed in explaining potential insights. However, it only introduces hyperparameters and possible combinatorial choices with no particular guidance in mind. For example, there are no experiments demonstrating the usefulness of this approach except for a toy mixture of Gaussians and binarized MNIST, explaining what is already known with the beta-VAE and infoGAN. It would be useful if the authors could make the paper overall more coherent and targeted to answer specific problems in the literature rather than try to encompass all of them. Misc + The ""feature marginal"" is also known as the aggregate posterior (Makhzani et al., 2015) and average encoding distribution (Hoffman and Johnson, 2016); also see Tomczak and Welling (2017)."	2	4	0.0432432432432432	0.9321	"Thank you for the feedback, I have read it. I do think that developing unifying frameworks is important. But not all unifying perspective is interesting; rather, a good unifying perspective should identify the behaviour of existing algorithms and inspire new algorithms. In this perspective, the proposed framework might be useful, but as noted in the original review, the presentation is not clear, and its not convincing to me that the MI framework is indeed useful in the sense I described above. I think probably the issue is the lack of good evaluation methods for generative models. Test-LL has no causal relationship to the quality of the generated data. So does MI. So I dont think the argument of preferring MI over MLE is convincing. So in summary, I will still keep my original score. I think the paper will be accepted by other venues if the presentation is improved and the advantage of the MI perspective is more explicitly demonstrated. ==== original review ==== Thank you for an interesting read. The paper presented a unifying framework for many existing generative modelling techniques, by first considering constrained optimisation problem of mutual information, then addressing the problem using Lagrange multipliers. I see the technical contribution to be the three theorems, in the sense that it gives a closure of all possible objective functions (if using the KL divergences). This can be useful: Im tired of reading papers which just add some extra ""regularisation terms"" and claim they work. I did not check every equation of the proof, but it seems correct to me. However, an imperfection is, the paper did not provide a convincing explanation on why their view should be preferred compared to the original papers intuition. For example in VAE case, why this mutual information view is better than the traditional view of approximate MLE, where q is known to be the approximate posterior? A better explanation on this (and similarly for say infoGAN/infoVAE) will significantly improve the paper. Continuing on the above point, why in section 4 you turn to discuss relationship between mutual information and test-LL? How does that relate to the main point you want to present in the paper, which is to prefer MI interpretation if I understand it correctly? Term usage: we usually *maximize* the ELBO and *minimise* the variational free-energy (VFE). "	3	4	0.0412371134020618	0.9895	2018
1	This paper presents an analysis of using multiple generators in a GAN setup, to address the mode-collapse problem. R1 was generally positive about the paper, raising the concern on how to choose the number of generators, and also whether parameter sharing was essential. The authors reported back on parameter sharing, showing its benefits yet did not have any principled method of selecting the number of generators. R2 was less positive about the paper, pointing out that mixture GANs and multiple generators have been tried before. They also raised concern with the (flawed) Inception score as the basis for comparison. R2 also pointed out that fixing the mixing proportions to uniform was an unrealistic assumption. The authors responded to these claims, clarifying the differences between this paper and the previous mixture GAN/multiple generator papers, and reporting FID scores. R3 was generally positive, also citing some novelty concerns similar to that of R2. I acknowledge the authors detailed responses to the reviews (in particular in response to R2) and I believe that the majority of concerns expressed have now been addressed. I also encourage the authors to include the FID scores in the final version of the paper.	"The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator, and an auxiliary classifier which predicts the source mixture component, plus a loss term which encourages diversity amongst components. All told the proposed method is quite incremental, as mixture GANs/multi-generators have been done before. The Inception scores are good but its widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. If the generator were to generate one template per class for which the Inception networks p(y|x) had low entropy, the Inception score would be quite high even though the model had only memorized one image per class. For claims surrounding mode collapse in particular, evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al (2017) would be the gold standard. Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score. The mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious. Finally, their own qualitative results indicate that theyve simply moved the problem, with clear evidence of mode collapse in one of their mixture components in figure 5c, 4th row from the bottom. Indeed, this does nothing to address the problem of mode collapse in general, as there is nothing preventing individual mixture component GANs from collapsing. Uncited prior work includes Generative Adversarial Parallelization of Im et al (2016). Also, if Im not mistaken this is quite similar to an AC-GAN, where the classes are instead randomly assigned and the generator conditioning is done in a certain way; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component. More discussion of this would be warranted. Other notes: - The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice. - ""As a result, the optimization order in 1 can be reversed"" this does not accurately characterize the source of the issues, see, e.g. Goodfellow (2015) ""On distinguishability criteria..."". - Section 3: the second last sentence of the third paragraph is vague and doesnt really say anything. Of course parameter sharing leverages common information. How does this help to train the model effectively? - Section 3: Since JSD is defined between two distributions, it is not clear what JSD_pi(P_G1, P_G2, ...) refers to. The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but its not clear what the steps are; it looks like a regular KL divergence to me. - Section 3: Also, is the classifier being trained to maximize this divergence or just the generator? I assume the latter. - The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi). - ""... which further minimizes the objective value"" -- it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and Im not sure why you bothered mentioning it. - Theres no mention of the substitution of log (1 - D(x)) for -log(D(x)) and its effect on the interpretation as a Jensen-Shannon divergence (which Im not sure was quite right in the first place) - Section 4: does the DAE introduced in DFM really introduce that much of a computational burden? - ""Symmetric Kullback Liebler divergence"" is not a well-known measure. The standard KL is asymmetric. Please define it. - Figure 2 is illegible in grayscale. - Improved-GAN score in Table 1 is misleading, as this was their no-label baseline. Its fine to include it but indicate it as such. Update: many of my concerns were adequately addressed, however I still feel that calling this an avenue to ""overcome mode collapse"" is misleading. This seems aimed at improving coverage of the support of the data distribution; test log likelihood bounds via AIS (there are GAN baselines for MNIST in the Wu et al manuscript I mentioned) would have been more compelling quantitative evidence. Ive raised my score to a 5."	2	4	0.034106412005457006	0.4152	"Summary: The paper proposes a mixture of generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added. Quality/clarity: The paper is well written and easy to follow. clarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text. Originality: Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator. General review: - when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z! - Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps? - in the tied weight case, in the synthetic example, can you show what each ""generator"" of the mixture learn? are they really learning modes of the data? - the theory is for general untied generators, can you comment on the tied case? I dont think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior. would be good to have some experiments and see how much we loose for example in term of inception scores, between tied and untied weights of generators. "	3	5	0.028037383177569996	0.9239	MGAN aims to overcome model collapsing problem by mixture generators. Compare to traditional GAN, there is a classifier added to minimax formulation. In training, MGAN is optimized towards minimizing the Jensen-Shannon Divergence between mixture distributions from generator and data distribution. The author also present that using MGAN to achive state-of-art results. The paper is easy to follow. Comment: 1. Seems there still no principle to choose correct number of generators but try different setting. Although most parameters of generators are shared, the result various. 2. Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing. 	3	3	0.0194174757281553	0.882	2018
1	This paper incorporates attention in the PixelCNN model and shows how to use MAML to enable few-shot density estimation. The paper received mixed reviews (7,6,4). After rebuttal the first reviewer updated the score to accept. The AC shares the concern of novelty with the first reviewer. However, it is also not trivial to incorporate attention and MAML in PixelCNN, thus the AC decided to accept the paper. 	"This paper focuses on few shot learning with autoregressive density estimation. Specifically, the paper improves PixelCNN with 1) neural attention, 2) meta learning techniques, and shows that the model achieve STOA few showt density estimation on the Omniglot dataset and demonstrate the few showt image generation on the Stanford Online Products dataset. The model is interesting, however, several details are not clear, which makes it harder to repeat the model and the experimental results. For example, what is the reason to use the (key, value) pair to encode these support images, what does the ""key"" means and what is the difference between ""keys"" and ""values""? In the experiments, the author did not explain the meaning of ""nats/dim"" and how to compute it. Another question is about the repetition of the experimental results. We know that PixelCNN is already a quite complicated model, it would be even harder to implement the proposed model. I wonder whether the author will release the official code to public to help the community?"	3	4	0.0357142857142857	0.8433	"This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning. The application is an obvious target for research and some relevant citations are missing, e.g. ""Towards a Neural Statistician"" (Edwards et al., ICLR 2017). Nonetheless, I think the current paper seems interesting enough to merit publication. The paper is well-produced, i.e. the overall writing, visuals, and narrative flow are good. It was easy to read the paper straight through while understanding both the technical details and more intuitive motivations. I have some concerns about the architectures and experiments presented in the paper. For architectures: the attention-based model seems powerful but difficult to scale to problems with more inputs for conditioning, and the meta PixelCNN model is a standard PixelCNN trained with the MAML approach by Finn et al. For experiments: the ImageNet flipping task is clearly tailored to the strengths of the attention-based model, and the presentation of the general Omniglot results could be improved. The image flipping experiment is neat, but the attention-based models strong performance is unsurprising. I think the results in Tables 1/2 should be merged into a single table. It would make it clear that the MAML-based and attention-based models achieve similar performance on this task. Overall, I think the paper makes a nice contribution. The paper could be improved significantly, e.g., by showing how to scale the attention-based architecture to problems with more data or by designing an architecture specifically for use with MAML-based inference."	3	4	0.0279999999999999	0.9817	"This paper focuses on the density estimation when the amount of data available for training is low. The main idea is that a meta-learning model must be learnt, which learns to generate novel density distributions by learn to adapt a basic model on few new samples. The paper presents two independent method. The first method is effectively a PixelCNN combined with an attention module. Specifically, the support set is convolved to generate two sets of feature maps, the so called ""key"" and the ""value"" feature maps. The key feature map is used from the model to compute the attention in particular regions in the support images to generate the pixels for the new ""target"" image. The value feature maps are used to copmpute the local encoding, which is used to generate the respective pixels for the new target image, taking into account also the attention values. The second method is simpler, and very similar to fine-tuning the basis network on the few new samples provided during training. Despite some interesting elements, the paper has problems. First, the novelty is rather limited. The first method seems to be slightly more novel, although it is unclear whether the contribution by combining different models is significant. The second method is too similar to fine-tuning: although the authors claim that mathcal{L}_inner can be any function that minimizes the total loss mathcal{L}, in the end it is clear that the log-likelihood is used. How is this approach (much) different from standard fine-tuning, since the quantity P(x; theta) is anyways unknown and cannot be ""trained"" to be maximized. Besides the limited novelty, the submission leaves several parts unclear. First, why are the convolutional features of the support set in the first methods divided into ""key"" and ""value"" feature maps as in p_key=p[:, 0:P], p_value=p[:, P:2*P]? Is this division arbitrary, or is there a more basic reason? Also, is there any different between key and value? Why not use the same feature map for computing the attention and computing eq (7)? Also, in the first model it is suggested that an additional feature can be having a 1-of-K channel for the supporting image label: the reason is that you might have multiple views of objects, and knowing which view contributes to the attention can help learning the density. However, this assumes that the views are ordered, namely that the recording stage has a very particular format. Isnt this a bit unrealistic, given the proposed setup anyways? Regarding the second method, it is not clear why leaving this room for flexibility (by allowing L_inner to be any function) to the model is a good idea. Isnt this effectively opening the doors to massive overfitting? Besides, isnt the statement that the function mathcal{L}_inner void? At the end of the day one can also claim the same for gradient descent: you dont need to have the true gradients of the true loss, as long as the objective function obtains gradually lower and lower values? Last, it is unclear what is the connection between the first and the second model. Are these two independent models that solve the same problem? Or are they connected? Regarding the evaluation of the models, the nature of the task makes the evaluation hard: for real data like images one cannot know the true distribution of particular support examples. Surrogate tasks are explored, first image flipping, then likelihood estimation of Omniglot characters, then image generation. Image flipping does not sound a very relevant task to density estimation, given that the task is deterministic. Perhaps, what would make more sense would be to generate a new image given that the support set has images of a particular orientation, meaning that the model must learn how to learn densities from arbitrary rotations. Regarding Omniglot character generation, the surrogate task of computing likelihood of known samples gives a bit better, however, this is to be expected when combining a model without attention, with an attention module. All in all, the paper has some interesting ideas. I encourage the authors to work more on their submission and think of a better evaluation and resubmit. "	3	5	0.0204379562043795	0.9925	2018
1	All reviewers agree that the proposed method is novel and experiments do a good job in establishing its value for few-shot learning. Most the concerns raised by the reviewers on experimental protocols have been addressed in the author response and revised version.	This paper introduces a graph neural net approach to few-shot learning. Input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node features. In addition to standard supervised few-shot classification, both semi-supervised and active learning task variants are introduced. The proposed approach captures several popular few-shot learning approaches as special cases. Experiments are conducted on both Omniglot and miniImagenet datasets. Strengths - Use of graph neural nets for few-shot learning is novel. - Introduces novel semi-supervised and active learning variants of few-shot classification. Weaknesses - Improvement in accuracy is small relative to previous work. - Writing seems to be rushed. The originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this paper. Graph neural nets seem to be a more natural way of representing sets of items, as opposed to previous approaches that rely on a random ordering of the labeled set, such as the FCE variant of Matching Networks or TCML. Others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future, and this paper represents a first step in that direction. Regarding the graph, I am wondering if the authors can comment on what scenarios is the graph structure expected to help? In the case of 1-shot, the graph can only propagate information about other classes, which seems to not be very useful. Though novel, the motivation behind the semi-supervised and active learning setup could use some elaboration. By including unlabeled examples in an episode, it is already known that they belong to one of the K classes. How realistic is this set-up and in what application is it expected that this will show up? For active learning, the proposed method seems to be specific to the case of obtaining a single label. How can the proposed method be scaled to handle multiple requested labels? Overall the paper is well-structured and related work covers the relevant papers, but the details of the paper seem hastily written. In the problem set-up section, it is not immediately clear what the distinction between s, r, and t is. Stating more explicitly that s is for the labeled data, etc. would make this section easier to follow. In addition, I would suggest stating the reason why t=1 is a necessary assumption for the proposed model in the few-shot and semi-supervised cases. Regarding the Omniglot dataset, Vinyals et al. (2016) augmented the classes so that 4,800 classes were used for training and 1,692 for test. Was the same procedure done for the experiments in the paper? If yes, please update 6.1.1 to make this distinction more clear. If not, please update the experiments to be consistent with the baselines. In the experiments, does the varphi MLP explicitly enforce symmetry and identity or is it learned? Regarding the Omniglot baselines, it appears that Koch et al. (2015), Edwards & Storkey (2016), and Finn et al. (2017) use non-standard class splits relative to the other methods. This should probably be noted. The results for Prototypical Networks appear to be incorrect in the Omniglot and Mini-Imagenet tables. According to Snell et al. (2017) they should be 49.4% and 68.2% for miniImagenet. Moreover, Snell et al. (2017) only used 64 classes for training instead of 80 as utilized in the proposed approach. Given this, I am wondering if the authors can comment on the performance difference in the 5-shot case, even though Prototypical Networks is a special case of GNNs? For semi-supervised and active-learning results, please include error bars for the miniImagenet results. Also, it would be interesting to see 20-way results for Omniglot as the gap between the proposed method and the baseline would potentially be wider. Other Comments: - In Section 4.2, Gc(.) is defined in Equation 2 but not mentioned in the text. - In Section 4.3, adding an equation to clarify the relationship with Matching Networks would be helpful. - I believe there is a typo in section 4.3 in that softmax(varphi) should be softmax(-varphi), so that more similar pairs will be more heavily weighted. - The equation in 5.1 appears to be missing a minus sign. Overall, the paper is novel and interesting, though the clarity and experimental results could be better explained. EDIT: I have read the authors response. The writing is improved and my concerns have largely been addressed. I am therefore revising my rating of the paper to a 7.	3	4	0.035667107001321	0.9929	This paper proposes to use graph neural networks for the purpose of few-shot learning, as well as semi-supervised learning and active learning. The paper first relies on convolutional neural networks to extract image features. Then, these image features are organized in a fully connected graph. Then, this graph is processed with an graph neural network framework that relies on modelling the differences between features maps, propto phi(abs(x_i-x_j)). For few-shot classification then the cross-entropy classification loss is used on the node. The paper has some interesting contributions and ideas, mainly from the point of view of applications, since the basic components (convnets, graph neural networks) are roughly similar to what is already proposed. However, the novelty is hurt by the lack of clarity with respect to the model design. First, as explained in 5.1 a fully connected graph is used (although in Fig. 2 the graph nodes do not have connections to all other nodes). If all nodes are connected to all nodes, what is the different of this model from a fully connected, multi-stream networks composed of S^2 branches? To rephrase, what is the benefit of having a graph structure when all nodes are connected with all nodes. Besides, what is the effect when having more and more support images? Is the generalization hurt? Second, it is not clear whether the label used as input in eq. (4) is a model choice or a model requirement. The reason is that the label already appears in the loss of the nodes in 5.1. Isnt using the label also as input redundant? Third, the paper is rather vague or imprecise at points. In eq. (1) many of the notations remain rather unclear until later in the text (and even then they are not entirely clear). For instance, what is s, r, t. The experimental section is also ok, although not perfect. The proposed method appears to have a modest improvement for few-shot learning. However, in the case of active learning and semi-supervised learning the method is not compared to any baselines (other than the random one), which makes conclusions hard to reach. In general, I tend to be in favor of accepting the paper if the authors have persuasive answers and provide the clarifications required.	3	4	0.0294906166219839	0.9029	This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors. The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al. This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks. The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art. There are a few typos and the presentation of the paper could be improved and polished more. I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot. I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. 	3	4	0.0111731843575419	0.9074	2018
1	This paper introduces a student-teacher method for learning from labels of varying quality (i.e. varying fidelity data). This is an interesting idea which shows promising results. Some further connections to various kinds of semi-supervised and multi-fidelity learning would strengthen the paper, although understandably it is not easy to cover the vast literature, which also spans different scientific domains. One reviewer had a concern about some design decisions that seemed ad-hoc, but at least the authors have intuitively and experimentally justified them.	"The problem of interest is to train deep neural network models with few labelled training samples. The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data. The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching. The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model. The teacher also supplies an uncertainty estimate to each predicted label. How about the heuristic function? This is used for learning initial feature representation of the student model. Crucially, the teacher model will also rely on these learned features. Labelled data and unlabelled data are therefore lie in the same dimensional space. Specific questions to be addressed: 1)	Clustering of strongly-labelled data points. Thinking about the statement “each an expert on this specific region of data space”, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points. Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data. On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited. As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations). It will be informative to provide results with a single GP model. 2)	From modifying learning rates to weighting samples. Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more “intuitive” to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty. Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). "	2	4	0.0083333333333333	0.6486	"This paper suggests a simple yet effective approach for learning with weak supervision. This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision. The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the ""fidelity"" of the weak label when training the student at the final step. The fidelity score is given by the teacher, after being trained over the clean data, and its used to build a cost-sensitive loss function for the students. The suggested method seems to work well on several document classification tasks. Overall, I liked the paper. I would like the authors to consider the following questions - - Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few). First, Id suggest acknowledging these works and discussing the differences to your work. Second - Is your approach applicable to these frameworks? It would be an interesting to compare to one of those methods (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve. - Can this approach be applied to semi-supervised learning? Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework? - The paper emphasizes that the teacher uses the students initial representation, when trained over the clean data. Is it clear that this step in needed? Can you add an additional variant of your framework when the fidelity score are computed by the teacher when trained from scratch? using different architecture than the student? - I went over the authors comments and I appreciate their efforts to help clarify the issues raised."	3	4	0.0286624203821656	0.9693	"The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data. This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable. The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework. The authors evaluate their proposed methods on one toy problem and two real-world problems. The paper is well written, easy to follow, and have good experimental study. My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work. Here are some questions that comes to my mind: (1) Why first building a student model only using the weak data and why not all the data together to train the student model? To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data? (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)? (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly? Why not learning the representation using an unsupervised learning method (unsupervised pre training)? This should be at least one of the baselines. (4) the idea of using surrogate labels to learn representation is also not new. One example work is ""Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks"". The authors didnt compare their method with this one."	3	3	0.0357142857142857	-0.6497	2018
1	This paper addresses an important application in genomics, i.e. the prediction of chromatin structure from nucleotide sequences. The authors develop a novel method for converting the nucleotide sequences to a 2D structure that allows a CNN to detect interactions between distant parts of the sequence. The reviewers found the paper innovative, interesting and convincing. Two reviewers gave a 7 and there was one 6. The 6, however, indicated during rather lengthy discussion that they were willing to raise their scores if their comments were addressed. Hopefully the authors will address these comments in the camera ready version. Overall a solid application paper with novel insights and technical innovation. 	There are functional elements attached on the DNA sequence, such as transcription factors and different kinds of histones as stated in this ms. A hidden assumption is that the binding sites of these functional elements over the genome share some common features. It is therefore biologically interesting to predict if a new DNA sequence could be a binding site. Naturally this is is classification problem where the input is the DNA sequence and the output is whether the give sequence is a binding site. This ms makes a novel way to transform the DNA sequence into a 3-dimensional tensor which could be easily utilised by CNN for images. The DNA sequence is first made into a a list of 4-mers. Then then each 4-mer is coded as a 4^4=256 dimensional vector. The order of the 4-mers is then coded into a image using Hilbert curve which presumably has nice properties to keep spatial information. I am not familiar with neural networks and do not comment on the methods but rather from the application point of view. First to my best knowledge, it is still controversial if the binding sites of different histones carries special features. I mean it could be possible that the assumption I mentioned in the beginning may not hold for this special application, especially for human data. I feel this method is more suitable for transcription factor motif data. see https://www.nature.com/articles/nbt.3300 Second, the experiments data in 2005 is measured using microarray, which uses probes of 500bp long. But the whole binding site for a nucleosome (or histone complex) is 147bp, which is much shorter than the probe. Nowadays we have more accurate sequencing data for nucleosome (check https://www.ncbi.nlm.nih.gov/pubmed/26411474). I am not sure whether this result will generalised to some other similar dataset. Third, the results only list the accuracy, it will be interesting to see the proportion of false negatives. In general I feel the transformation is quite useful, it nicely reserves the spatial information, also can be seen from the improved results over all datasets. The result, in my opinion, is not sufficient to support the assumption that we could predict the DNA structures solely base on the sequence. 	3	3	0.0439560439560439	0.9825	The authors of this manuscript transformed the k-mer representation of DNA fragments to a 2D image representation using the space-filling Hilbert curves for the classification of chromatin occupancy. In generally, this paper is easy to read. The components of the proposed model mainly include Hilbert curve theory and CNN which are existing technologies. But the authors make their combination useful in applications. Some specific comments are: 1. In page 5, I could not understand the formula d_kink < d_out. d_link ; 2. There may exist some new histone modification data that were captured by the next-generation sequencing (e.g. ChIP-seq) and are more accurate; 3. It seems that the authors treat it as a two-class problem for each data set. It would be more useful in real applications if all the data sets are combined to form a multi-class problem. 	3	5	0.0214285714285714	0.7184	Dear editors, the authors addressed all of my comments and clearly improved their manuscript over multiple iterations. I therefore increased my rating from ‘6: Marginally above acceptance threshold’ to ‘7: Good paper, accept’. Please note that the authors made important edits to their manuscript after the ICLR deadline and could hence not upload their most current version, which you can from https://file.io/WIiEw9. If you decided to publish the manuscript, I hence highly suggest using this (https://file.io/WIiEw9) version. Best, -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- The authors present Hilbert-CNN, a convolutional neural network for DNA sequence classification. Unlike existing methods, their model does not use the raw one-dimensional (1D) DNA sequence as input, but two-dimensional (2D) images obtained by mapping sequences to images using spacing-filling Hilbert-Curves. They further present a model (Hilbert-CNN) that is explicitly designed for Hilbert-transformed DNA sequences. The authors show that their approach can increase classification accuracy and decrease training time when applied to predicting histone-modification marks and splice junctions. Major comments ============= 1. The motivation of transforming sequences into images is unclear and claimed benefits are not sufficiently supported by experiments. The essence of deep neural networks is to learn a hierarchy of features from the raw data instead of engineering features manually. Using space filling methods such as Hilbert-curves to transform (DNA) sequences into images can be considered as unnecessary feature-engineering. The authors claim that ‘CNNs have proven to be most powerful when operating on multi-dimensional input, such as in image classification’, which is wrong. Sequence-based convolutional and recurrent models have been successfully applied for modeling natural languages (translation, sentiment classification, …), acoustic signals (speech recognition, audio generation), or biological sequences (e.g. predicting various epigenetic marks from DNA as reviewed in Angermueller et al). They further claim that their method can ‘better take the spatial features of DNA sequences into account’ and can better model ‘long-term interactions’ between distant regions. This is not obvious since Hilbert-curves map adjacent sequence characters to pixels that are close to each other as described by the authors, but distant characters to distant pixels. Hence, 2D CNN must be deep enough for modeling interactions between distant image features, in the same way as a 1D CNN. Transforming sequences to images has several drawbacks. 1) Since the resulting images have a small width and height but many channels, existing 2D CNNs such as ResNet or Inception can not be applied, which also required the authors to design a specific model (Hilbert-CNN). 2) Hilbert-CNN requires more memory due to empty image regions. 3) Due to the high number of channels, convolutional filters have more parameters. 4) The sequence-to-image transformation makes model-interpretability hard, which is in particular important in biology. For example, motifs of the first convolutional layers can not be interpreted as sequence motifs (as described in Angermueller et al) and it is unclear how to analyze the influence of sequence characters using attention or gradient-based methods. The authors should more clearly motivate their model in the introduction, tone-down the benefit of sequence-to-image transformations, and discuss drawbacks of their model. This requires major changes of introduction and discussion. 2. The authors should more clearly describe which and how they optimized hyper-parameters. The authors should optimize the most important hyper-parameters of their model (learning rate, batch size, weight decay, max vs. average pooling, ELU vs. ReLU, …) and baseline models on a holdout validation set. The authors should also report the validation accuracy for different sequence lengths, k-mer sizes, and space filling functions. Can their model be applied to longer sequences (>= 1kbp) which had been shown to improve performance (e.g. 10.1101/gr.200535.115)? Does Figure 4 show the performance on the training, validation, or test set? 3. It is unclear if the performance gain is due the proposed sequence-to-image transformation, or due to the proposed network architecture (Hilbert-CNN). It is also unclear if Hilbert-CNNs are applicable to DNA sequence classification tasks beyond predicting chromatin states and splice junctions. To address these points, the authors should compare Hilbert-CNN to models of the same capacity (number of parameters) and optimize hyper-parameters (k-mer size, convolutional filter size, learning rate, …) in the same way as they did for Hilbert-CNN. The authors should report the number of parameters of all models (Hilbert-CNN, Seq-CNN, 1D-sequence-CNN (Table 5), and LSTM (Table 6), …) in an additional table. The authors should also compare Hilbert-CNN to the DanQ architecture on predicting epigenetic markers using the same dataset as reported in the DanQ publication (DOI: 10.1093/nar/gkw226). The authors should also compare Hilbert-CNNs to gapped-kmer SVM, a shallow model that had been successfully applied for genomic prediction tasks. 4. The authors should report the AUC and area under precision-recall curve (APR) in additional to accuracy (ACC) in Table 3. 5. It is unclear how training time was measured for baseline models (Seq-CNN, LSTM, …). The authors should use the same early stopping criterion as they used for training Hilber-CNNs. The authors should also report the training time of SVM and gkm-SVM (see comment 3) in Table 3. Minor comments ============= 1. The authors should avoid uninformative adjectives and clutter throughout the manuscript, for example ‘DNA is often perceived’, ‘Chromatin can assume’, ‘enlightening’, ‘very’, ‘we first have to realize’, ‘do not mean much individually’, ‘very much like the tensor’, ‘full swing’, ‘in tight communication’, ‘two methods available in the literature’. The authors should point out in section two that k-mers can be overlapping. 2. Section 2.1: One-hot vectors is not the only way for embedding words. The authors should also mention Glove and word2vec. Similar approaches had been applied to protein sequences (DOI: 10.1371/journal.pone.0141287) 3. The authors should more clearly describe how Hilbert-curves map sequences to images and how images are cropped. What does ‘that is constructed in a recursive manner’ mean? Simply cropping the upper half of Figure 1c would lead to two disjoint sequences. What is the order of Figure 1e? 4. The authors should consistently use ‘channels’ instead of ‘full vector of length’ to denote the dimensionality of image pixels. 5. The authors should use ‘Batch norm’ instead of ‘BN’ in Figure 2 for clarification. 6. Hilber-CNN is similar to ResNet (DOI: 10.1371/journal.pone.0141287), which consists of multiple ‘residual blocks’, where each block is a sequence of ‘residual units’. A ‘computational block’ in Hilbert-CNN contains two parallel ‘residual blocks’ (Figure 3) instead of a sequence of ‘residual units’. The authors should use ‘residual block’ instead of ‘computational block’, and ‘residual units’ as in the original ResNet publication. The authors should also motivate why two residual units/blocks are applied parallely instead of sequentially. 7. Caption table 1: the authors should clarify if ‘Output size’ is ‘height, width, channels’, and explain the notation in ‘Description’ (or refer to the text.)	3	5	0.0279531109107303	0.9963	2018
0	All reviewers believed that the novelty of the contribution was limited.	I will be upfront: I have already reviewed this paper when it was submitted to NIPS 2017, so this review is based heavily on the NIPS submission. I am quite concerned that this paper has been resubmitted as it is, word by word, character by character. The authors could have benefited from the feedback they obtained from the reviewers of their last submissions to improved their paper, but nothing has been done. Even very easy remarks, like bolding errors (see below) have been kept in the paper. The proposed paper describes a method for video action segmentation, a task where the video must be temporally densely labeled by assigned an action (sub) class to each frame. The method proceeds by extracting frame level features using convolutional networks and then passing a temporal encoder-decoder in 1D over the video, using fully supervised training. On the positive side, the method has been tested on 3 different datasets, outperforming the baselines (recent methods from 2016) on 2 of them. My biggest concern with the paper is novelty. A significant part of the paper is based on reference [Lea et al. 2017], the differences being quite incremental. The frame-level features are the same as in [Lea et al. 2017], and the basic encoder-decoder strategy is also taken from [Lea et al. 2017]. The encoder is also the same. Even details are reproduced, as the choice of normalized Relu activations. The main difference seems to me that the decoder is not convolutional, but a recurrent network. The encoder-decoder architecture seems to be surprisingly shallow, with only K=2 layers at each side. The paper is well written and can be easily understood. However, a quite large amount of space is wasted on obvious and known content, as for example the basic equation for a convolutional layer (equation (1)) and the following half page of text and equations of LSTM and Bi-directional LSTM networks. This is very well known and the space can be used for more details on the papers contributions. While the paper is generally well written, there are a couple of exceptions in the form of ambiguous sentences, for example the lines before section 3. There is a bolding error in table 2, where the proposed method is not state of the art (as indicated) w.r.t. to the accuracy metric. To sum it up, the positive aspect of nicely executed experiments is contrasted by low novelty of the method. To be honest, I am not totally sure whether the contribution of the paper should be considered as a new method or as architectural optimizations of an existing one. This is corroborated by the experimental results on the first two datasets (tables 2 and 3): on 50 salads, where ref. [Lea et al. 2017]. seems currently to obtain state of the art performance, the improvement obtained by the proposed method allows it to get state of the art performance. On GTEA, where [Lea et al. 2017] does not currently deliver state of the art performance, the proposed method performs (slightly) better than [Lea et al. 2017] but does not obtain state of the art performance. On the third dataset, JIGSAWS, reference [Lea et al. 2017]. has not been tested, which is peculiar given the closeness. 	1	5	0.0166358595194085	0.9907	"This paper discusses the problem of action segmentation in long videos, up to 10 minutes long. The basic idea is to use a temporal convolutional encoder-decoder architecture, where in the enconder 1-D temporal convolutions are used. In the decoder three variants are studied: (1) One that uses only several bidirectional LSTMs, one after the other. (2) One that first applies successive layers of deconvolutions to produce per frame feature maps. Then, in the end a bidirectional LSTM in the last layer. (3) One that first applies a bidirectional LSTM, then applies successively 1-D deconvolution layer. All variants end with a ""temporal softmax"" layer, which outputs a class prediction per frame. Overall, the paper is of rather limited novelty, as it is very similar to the work of Lea et al., 2017, where now the decoder part also has the deconvolutions smoothened by (bidirectional) LSTMs. It is not clear what is the main novelty compared to the aforementioned paper, other than temporal smoothing of features at the decoder stage. Although one of the proposed architectures (TricorNet) produces some modest improvements, it is not clear why the particular architectures are a good fit. Surely, deconvolutions and LSTMs can help incorporate some longer-term temporal elements into the final representations. However, to begin with, arent the 1-D deconvolutions and the LSTMs (assuming they are computed dimension-wise) serving the same purpose and therefore overlapping? Why are both needed? Second, what makes the particular architectures in Figure 3 the most reasonable choice for encoding long-term dependencies, is there a fundamental reason? What is the difference of the L_mid from the 1-D deconv layers afterward? Currently, the three variants are motivated in terms of what the Bi-LSTM can encode (high or low level details). Third, the qualitative analysis can be improved. For instance, the experiment with the ""cut lettuce"" vs ""peel cucumber"" is not persuasive enough. Indeed, longer temporal relationships can save incorrect future predictions. However, this works both ways, meaning that wrong past predictions can persist because of the long-term modelling. Is there a mechanism in the proposed approach to account for that fact? All in all, I believe the paper indeed improves over existing baselines. However, the novelty is insufficient for a publication at this stage."	1	5	0.0188679245283018	0.9336	The paper proposed a combination of temporal convolutional and recurrent network for video action segmentation. Overall this paper is written and easy to follow. The novelty of this paper is very limited. It just replaces the decoder of ED-TCN (Lea et al. 2017) with a bi-directional LSTM. The idea of applying bi-directional LSTM is also not new for video action segmentation. In fact, ED-TCN used it as one of the baselines. The results also do not show much improvement over ED-TCN, which is much easier and faster to train (as it is fully convolutional model) than the proposed model. Another concern is that the number of layers parameter K. The authors should show an analysis on how the performance varies for different values of K which I believe is necessary to judge the generalization of the proposed model. I also suggest to have an analysis on entire convolutional model (where the decoder has 1D-deconvolution) to be included in order to get a clear picture of the improvement in performance due to bi-directional LSTM . Overall, I believe the novelty, contribution and impact of this work is sub-par to what is expected for publication in ICLR. 	2	4	0.0255102040816326	0.6306	2018
1	Novel way of analyzing neural networks to predict NN attributes such as architecture, training method, batch size etc. And the method works surprisingly good on the MNIST and ImageNet.	The paper attempts to study model meta parameter inference e.g. model architecture, optimization, etc using a supervised learning approach. They take three approaches one whereby the target models are evaluated on a fixed set of inputs, one where the access to the gradients is assumed and using that an input is crafted that can be used to infer the target quantities and one where both approaches are combined. The authors also show that these inferred quantities can be used to generate more effective attacks against the targets. The paper is generally well written and most details for reproducibility are seem enough. I also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intriguing. It is of course not entirely surprising that the system can be trained but that there is some form of generalization happening. Aside that I think most system in practical use will be much more different than any a priori enumeration/brute force search for model parameters. I suspect in most cases practical systems will be adapted with many subsequent levels of preprocessing, ensembling, non-standard data and a number of optimization and architectural tricks that are developer dependent. It is really hard to say what a supervised learning meta-model approach such as the one presented in this work have to say about that case. I have found it hard to understand what table 3 in section 4.2 actually means. It seems to say for instance that a model is trained on 2 and 3 layers then queried with 4 and the accuracy only slightly drops. Accuracy of what ? Is it the other attributes ? Is it somehow that attribute ? if so how can that possibly ? My main main concern is extrapolation out of the training set which is particularly important here. I dont find enough evidence in 4.2 for that point. One experiment that i would find compelling is to train for instance a meta model on S,V,B,R but not D on imagenet, predict all the attributes except architecture and see how that changes when D is added. If these are better than random and the perturbations are more successful it would be a much more compelling story. 	2	4	0.0263852242744063	0.9535	" -----UPDATE------ Having read the responses from the authors, and the other reviews, I am happy with my rating and maintain that this paper should be accepted. ---------------------- In this paper, the authors trains a large number of MNIST classifier networks with differing attributes (batch-size, activation function, no. layers etc.) and then utilises the inputs and outputs of these networks to predict said attributes successfully. They then show that they are able to use the methods developed to predict the family of Imagenet-trained networks and use this information to improve adversarial attack. I enjoyed reading this paper. It is a very interesting set up, and a novel idea. A few comments: The paper is easy to read, and largely written well. The article is missing from the nouns quite often though so this is something that should be amended. There are a few spelling slip ups (""to a certain extend"" --> ""to a certain extent"", ""as will see"" --> ""as we will see"") It appears that the output for kennen-o is a discrete probability vector for each attribute, where each entry corresponds to a possibility (for example, for ""batch-size"" it is a length 3 vector where the first entry corresponds to 64, the second 128, and the third 256). What happens if you instead treat it as a regression task, would it then be able to hint at intermediates (a batch size of 96) or extremes (say, 512). A flaw of this paper is that kennen-i and io appear to require gradients from the network being probed (you do mention this in passing), which realistically you would never have access to. (Please do correct me if I have misunderstood this) It would be helpful if Section 4 had a paragraph as to your thoughts regarding why certain attributes are easier/harder to predict. Also, the caption for Table 2 could contain more information regarding the network outputs. You have jumped from predicting 12 attributes on MNIST to 1 attribute on Imagenet. It could be beneficial to do an intermediate experiment (a handful of attributes on a middling task). I think this paper should be accepted as it is interesting and novel. Pros ------ - Interesting idea - Reads well - Fairly good experimental results Cons ------ - kennen-i seems like it couldnt be realistically deployed - lack of an intermediate difficulty task "	3	4	0.0204081632653061	0.9896	The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach). It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy. As a simple example, its possible that there are values of batch size for which the classifiers may become indistinguishable, yet Table 2 shows that batch size can be predicted with much higher accuracy than chance. It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately. That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization. The selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries? One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy. In table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io. In the ImageNet classifier family prediction, how different are the various families from each other? Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader. Overall the results seem interesting, but without more insights its difficult to judge how generally useful they are.	3	3	0.0477941176470588	0.8374	2018
1	The reviewers are unanimous in accepting the paper. They generally view it as introducing an original approach to online RL using bandit-style selection from a fixed portfolio of off-policy algorithms. Furthermore, rigorous theoretical analysis shows that the algorithm achieves near-optimal performance. The only real knock on the paper is that they use a weak notion of regret i.e. short-sighted pseudo regret. This is considered inevitable, given the setting.	"The authors consider the problem of dynamically choosing between several reinforcement learning algorithms for solving a reinforcement learning with discounted rewards and episodic tasks. The authors propose the following solution to the problem: - During epochs of exponentially increasing size (this technique is well known in the bandit litterature and is called a ""doubling trick""), the various reinforcement learning algorithms are ""frozen"" (i.e. they do not adapt their policy) and the K available algorithms are sampled using the UCB1 algorithm in order to discover the one which yields the highest mean reward. Overall the paper is well written, and presents some interesting novel ideas on aggregating reinforcement learning algorithms. Below are some remarks: - An alternative and perhaps simpler formalization of the problem would be learning with expert advice (using algorithms such as ""follow the perturbed leader""), where each of the available reinforcement learning algorithms acts as an expert. What is more, these algorithms usually yield O(sqrt(T)log(T)), which is the regret obtained by the authors in the worse case (where all the learning algorithms do converge to the optimal policy at the optimal speed O(1/sqrt(T)). It would have been good to see how those approaches perform against the proposed algorithms. - The authors use UCB1, but they did not try KL-UCB, which is stricly better (in fact it is optimal for bounded rewards). In particular the numerical performance of the latter is usually vastly better than the former, especially when rewards have a small variance. - The performance measure used by the authors is rather misleading (""short sighted regret""): they compare what they obtain to what the policy discovered by the best reainforcement learning algorithm underline{based on the trajectories they have seen}, and the trajectories themselves are generated by the choices made by the algorthms at previous time. Ie in general, there might be cases in which one does not explore enough with this approach (i.e one does not try all state-action pairs enough), so that while this performance measure is low, the actual regret is very high and the algorithm does not learn the optimal policy at all (while this could be done by simply exploring at random log(T) times ...). "	3	3	0.0082644628099173	0.9709	"SUMMARY The paper considers a meta-algorithm, in the form of a UCB algorithms, that selects base-learners in a pool of reinforcement learning agents. HIGH LEVEL COMMENTS In this paper, T refers to the total number of meta-decisions. This is very different from the total number of interactions with the system that corresponds to D_T=sum_{tau=1}^T |epsilon_tau|. Wouldnt it make more sense to optimize regret accumulated on this global time? The proposed strategy thus seems a bit naive since different algorithms from the set cal P may generate trajectories of different length. For instance, one algorithm may obtain relatively high rewards very fast with short trajectories and another one may get slightly higher cumulative rewards but on much longer trajectories. In this case, the meta-algorithm will promote the second algorithm, while repeatedly selecting the first one would yield higher cumulative reward in total over all decision (and not meta-decision) time steps. This also means that playing T1 meta-decision steps, where T1>>T, may corresponds to a total number of decision steps sum_{tau=1}^{T_1} |epsilon_tau| still not larger than D_T (where epsilon_tau are other trajectories). Now the performance of a specific learner with T1 trials may be much higher than with T trials, and thus even though the regret of the meta-learner is higher, the overall performance of the recommended policy learned at that point may be better than the one output with T meta-decisions. Thus, it seems to me that a discussion about the total number of decision steps (versus meta-deciion steps) is missing in order to better motivate the choise of performance measure, and generates possibly complicated situations, with a non trivial trade-off that needs to be adressed. This also suggests the proposed algorithm may be quite sub-optimal in terms of total number of decision steps. My feeling is that the reason you do not observe a too bad behavior in practice may be due to the discount factor. OTHER COMMENTS: Page 4: What value of xi do you use in ESBAS ? I guess it should depend on R/(1-gamma)? Page 5: ""one should notice that the first two bounds are obtained by summming up the gaps"": which bounds? which gaps? Can you be more precise? Next sentence also needs to be clarified. What is the budget issue involved here? Can you comment on the main reason why you indeed get O(sqrt{T}) and not O(sqrt{T poly-log(T)}) for instance? Theorem 3: ""with high probability delta_T in O(1/T)"": do you mean with probability higher than 1-delta_T, with delta_T = O(1/T) ? Paragraph on Page 15 : Do you have a proof for the claim that such algorithms indeed satisfy these assumptions ? Especially proving that assumption 3 holds may not be obvious since one may consider an algorithm may better learn using data collected from its played polocy rather than from other policies. (14b-c, 15d): should u be u^alpha ? I may simply be confused with the notations. DECISION Even though there seems to be an important missing discussion regarding optimization of performance with respect to the total number of decision steps rather than the total number of meta-decision steps, I would tend to accept the paper. Indeed, if we left apart the choice for this performance measure, the paper is relatively well written and provides both theoretical and practical results that are of interest. But this has to be clarified. "	3	5	0.0484739676840215	0.9958	The paper considers the problem of online selection of RL algorithms. An algorithm selection (AS) strategy called ESBAS is proposed. It works in a sequence of epochs of doubling length, in the following way: the algorithm selection is based on a UCB strategy, and the parameters of the algorithms are not updated within each epoch (in order that the returns obtained by following an algorithm be iid). This selection allows ESBAS to select a sequence of algorithms within an epoch to generate a return almost as high as the return of the best algorithm, if no learning were made. This weak notion of regret is captured by the short-sighted pseudo regret. Now a bound on the global regret is much harder to obtain because there is no way of comparing, without additional assumption, the performance of a sequence of algorithms to the best algorithm had this one been used to generate all trajectories from the beginning. Here it is assumed that all algorithms learn off-policy. However this is not sufficient, since learning off-policy does not mean that an algorithm is indifferent to the behavior policy that has generated the data. Indeed even for the most basic off-policy algorithms, such as Q-learning, the way data have been collected is extremely important, and collecting transitions using that algorithm (such as epsilon-greedy) is certainly better than following an arbitrary policy (such as uniformly randomly, or following an even poorer policy which would not explore at all). However the authors seem to state an equivalence between learning off-policy and fairness of learning (defined in Assumption 3). For example in their conclusion they mention “Fairness of algorithm evaluation is granted by the fact that the RL algorithms learn off-policy”. This is not correct. I believe the main assumption made in this paper is the Assumption 3 (and not that the algorithms learn off-policy) and this should be dissociated from the off-policy learning. This fairness assumption is very a strong assumption that does not seem to be satisfied by any algorithm that I can think of. Indeed, consider D being the data generated by following algorithm alpha, and let D’ be the data generated by following algorithm alpha’. Then it makes sense that the performance of algorithm alpha is better when trained on D rather than D’, and alpha’ is better when trained on D’ than on D. This contradicts the fairness assumption. This being said, I believe the merit of this paper is to make explicit the actual assumptions required to be able to derive a bound on the global regret. So although the fairness assumption is unrealistic, it has the benefit of existing… So in the end I liked the paper because the authors tried to address this difficult problem of algorithmic selection for RL the best way they could. Maybe future work will do better but at least this a first step in an interesting direction. Now, I would have liked a comparison with other algorithms for algorithm selection, like: - explore-then-exploit, where a fraction of T is used to try each algorithm uniformly, then the best one is selected and played for the rest of the rounds. - Algorithms that have been designed for curriculum learning, such as some described in [Graves et al., Automated Curriculum Learning for Neural Networks, 2017], where a proxy for learning progress is used to estimate how much an algorithm can learn from data. Other comments: - Table 1 is really incomprehensible. Even after reading the Appendix B, I had a hard time understanding these results. - I would suggest adding the fairness assumption in the main text and discussing it, as I believe this is crucial component for the understanding of how the global regret can be controlled. - you may want to include references on restless bandits in Section 2.4, as this is very related to AS of RL algorithms (arms follow Markov processes). - The reference [Best arm identification in multi-armed bandits] is missing an author. 	3	4	0.0316742081447963	0.9709	2018
1	This paper compares autoencoder and GAN-based methods for 3D point cloud representation and generation, as well as new (and welcome) metrics for quantitatively evaluating generative models. The experiments form a good but still a bit too incomplete exploration of this topic. More analysis is needed to calibrate the new metrics. Qualitative analysis would be very helpful here to complement and calibrate the quantitative ones. The writing also needs improvement for clarity and verbosity. The author replies and revisions are very helpful, but there is still some way to go on the issues above. Overall, the committee is intersting and recommends this paper for the workshop track.	This paper introduces a generative approach for 3D point clouds. More specifically, two Generative Adversarial approaches are introduced: Raw point cloud GAN, and Latent-space GAN (r-GAN and l-GAN as referred to in the paper). In addition, a GMM sampling + GAN decoder approach to generation is also among the experimented variations. The results look convincing for the generation experiments in the paper, both from class-specific (Figure 1) and multi-class generators (Figure 6). The quantitative results also support the visuals. One question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approaches. Especially Octree based approaches [1-below] show very convincing and high-resolution shape generation results, whereas the details seem to be washed out for the point cloud results presented in this paper. I would like to see comparison experiments with voxel based approaches in the next update for the paper. [1] @article{tatarchenko2017octree, title={Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs}, author={Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas}, journal={arXiv preprint arXiv:1703.09438}, year={2017} } In light of the authors octree updates score is updated. I expect these updates to be reflected in the final version of the paper itself as well. 	3	5	0.0248756218905472	0.9371	3D data processing is very important topic nowadays, since it has a lot of applications: robotics, AR/VR, etc. Current approaches to 2D image processing based on Deep Neural Networks provide very accurate results and a wide variety of different architectures for image modelling, generation, classification, retrieval. The lack of DL architectures for 3D data is due to complexity of representation of 3D data, especially when using 3D point clouds. Considered paper is one of the first approaches to learn GAN-type generative models. Using PointNet architecture and latent-space GAN, the authors obtained rather accurate generative model. The paper is well written, results of experiments are convincing, the authors provided the code on the github, realizing their architectures. Thus I think that the paper should be published.	4	5	0.024	0.5563	"Summary: This paper proposes generative models for point clouds. First, they train an auto-encoder for 3D point clouds, somewhat similar to PointNet (by Qi et al.). Then, they train generative models over the auto-encoders latent space, both using a ""latent-space GAN"" (l-GAN) that outputs latent codes, and a Gaussian Mixture Model. To generate point clouds, they sample a latent code and pass it to the decoder. They also introduce a ""raw point cloud GAN"" (r-GAN) that, instead of generating a latent code, directly produces a point cloud. They evaluate the methods on several metrics. First, they show that the autoencoders latent space is a good representation for classification problems, using the ModelNet dataset. Second, they evaluate the generative model on several metrics (such as Jensen-Shannon Divergence) and study the benefits and drawbacks of these metrics, and suggest that one-to-one mapping metrics such as earth movers distance are desirable over Chamfer distance. Methods such as the r-GAN score well on the latter by over-representing parts of an object that are likely to be filled. Pros: - It is interesting that the latent space models are most successful, including the relatively simple GMM-based model. Is there a reason that these models have not been as successful in other domains? - The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs. Due to the simplicity of the method, this paper could be a useful baseline for future work. - The part-editing and shape analogies results are interesting, and it would be nice to see these expanded in the main paper. Cons: - How does a model that simply memorizes (and randomly samples) the training set compare to the auto-encoder-based models on the proposed metrics? How does the diversity of these two models differ? - The paper simultaneously proposes methods for generating point clouds, and for evaluating them. The paper could therefore be improved by expanding the section comparing to prior, voxel-based 3D methods, particularly in terms of the diversity of the outputs. Although the performance on automated metrics is encouraging, it is hard to conclude much about under what circumstances one representation or model is better than another. - The technical approach is not particularly novel. The auto-encoder performs fairly well, but it is just a series of MLP layers that output a Nx3 matrix representing the point cloud, trained to optimize EMD or Chamfer distance. The most successful generative models are based on sampling values in the auto-encoders latent space using simple models (a two-layer MLP or a GMM). - While it is interesting that the latent space models seem to outperform the r-GAN, this may be due to the relatively poor performance of r-GAN than to good performance of the latent space models, and directly training a GAN on point clouds remains an important problem. - The paper could possibly be clearer by integrating more of the ""background"" section into later sections. Some of the GAN figures could also benefit from having captions. Overall, I think that this paper could serve as a useful baseline for generating point clouds, but I am not sure that the contribution is significant enough for acceptance. "	2	4	0.0207939508506616	0.9929	2018
0	"In principle, the idea behind the submission is sound: use a generative model (GANs in this case) to learn to generate desirable ""goals"" (subsets of the state space) and use that instead of uniform sampling for goals. Overall I tend to agree with Reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a high-dimensional state space, which seems to be be whole raison detre of GANs in this proposed method. The coverage experiment in Figure 5 seems like a good *illustration* of the method, but for this work to be convincing, I think we would need a more diverse set of experiments (a la Figure 2) showing how this method performs on complicated tasks. I encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the Assumptions being made in Section 3.3 are *violated* somehow (to actually test how the method fails in those cases)."	"In general I find this to be a good paper and vote for acceptance. The paper is well-written and easy to follow. The proposed approach is a useful addition to existing literature. Besides that I have not much to say except one point I would like to discuss: In 4.2 I am not fully convinced of using an adversial model for goal generation. RL algorithms generally suffer from poor stability and GANs themselves can have convergence issues. This imposes another layer of possible instability. Besides, generating useful reward function, while not trivial, can be seen as easier than solving the full RL problem. Can the authors argue why this model class was chosen over other, more simple, generative models? Furthermore, did the authors do experiments with simpler models? Related: ""We found that the LSGAN works better than other forms of GAN for our problem."" Was this improvement minor, or major, or didnt even work with other GAN types? This question is important, because for me the big question is if this model is universal and stable in a lot of applications or requires careful fine-tuning and monitoring. --- Update: The authors addressed the major point of criticism in my review. I am now more convinced in the quality of the proposed work, and have updated my review score accordingly."	4	4	0.0410958904109589	0.9491	This paper proposed a method for automatic curriculum generation that allow an agent to learn to reach multiple goals in an environment with considerable sample efficiency. They use a generator network to propose tasks for the agent accomplish. The generator network is trained with GAN. In addition, the proposed method is also shown to be able to solve tasks with sparse rewards without the need manually modify reward functions. They compare the Goal GAN method with four baselines, including Uniform sampling, Asymmetric Self-play, SAGG-RIAC, and Rejection sampling. The proposed method is tested on two environments: Free Ant and Maze Ant. The empirical study shows that the proposed method is able to improve policies’ training efficiency comparing to these baselines. The technical contributions seem sound, however I find it is slightly difficult to fully digest the whole paper without getting the insight from each individual piece and there are some important details missing, as I will elaborate more below. 1. it is unclear to me why the proposed method is able to solve tasks with sparse rewards? Is it because of the horizons of the problems considered are not long enough? The author should provide more insight for this contribution. 2. It is unclear to me how R_min and R_max as hyperparameters are obtained and how their settings affect the performance. 3. Another concern I have is regarding the generalizability of the proposed method. One of the assumption is “A policy trained on a sufficient number of goals in some area of the goal-space will learn to interpolate to other goals within that area”. This seems to mean that the area is convex. It might be better if some quantitative analysis can be provided to illustrate geometry of goal space (given complex motor coordination) that is feasible for the proposed method. 4. It is difficult to understand the plots in Figure 4 without more details. Do you assume for every episode, the agent starts from the same state? 5. For the plots in Figure 2, is there any explanation for the large variance for Goal GAN? Given that the state space is continuous, 10 runs seems not enough. 6. According to the experimental details, three rollouts are performed to estimate the empirical return. It there any justification why three rollouts are enough? 7. Minor comments Achieve tasks -> achieve goals or accomplish/solve tasks A variation of to -> variation of Allows a policy to quickly learn to reach …-> allow an agent to be quickly learn a policy to reach… …the difficulty of the generated goals -> … the difficulty of reaching 	3	4	0.0185185185185185	0.9489	"Summary: This paper proposes to use a GAN to generate goals to implement a form of curriculum learning. A goal is defined as a subset of the state space. The authors claim that this model can discover all ""goals"" in the environment and their difficulty, which can be measured by the success rate / reward of the policy. Hence the goal network could learn a form of curriculum, where a goal is good if it is a state that the policy can reach after a (small) improvement of the current policy. Training the goal GAN is done via labels, which are states together with the achieved reward by the policy that is being learned. The benchmark problems are whether the GAN generates goals that allow the agent to reach the end of a U-maze, and a point-mass task. Authors compare GAN goal generation vs uniformly choosing a goal and 2 other methods. My overall impression is that this work addresses an interesting question, but the experimental setup / results are not clearly worked out. More broadly, the paper does not address how one can combine RL and training a goal GAN in a stable way. Pro: - Developing hierarchical learning methods to improve the sample complexity of RL is an important problem. - The paper shows that the U-maze can be solved using a variety of methods that generate goals in a non-uniform way. Con: - It is not clear to me how the asymmetric self-play and SAGG-RIAC are implemented and why they are natural baselines. - It is not clear to me what the goals are in the point mass experiment. This entire experiment should be explained much more clearly (+image). - It is not clear how this method compares qualitatively vs baselines (differences in goals etc). - This method doesnt seem to always outperform the asymm-selfplay baseline. The text mentions that baseline is less efficient, but this doesnt make the graph very interpretable. - The curriculum in the maze-case consists of regions that just progress along the maze, and hence is a 1-dimensional space. Hence using a manually defined set of goals should work quite well. It would be better to include such a baseline as well. - The experimental maze-setting and point-mass have a simple state / goal structure. How can this method generalize to harder problems? -- The entire method is quite complicated (e.g. training GANs can be highly unstable). How do we stabilize / balance training the GAN vs the RL problem? -- I dont see how this method could generalize to problems where the goals / subregions of space do not have a simple distribution as in the maze problem, e.g. if there are multiple ways of navigating a maze towards some final goal state. In that case, to discover a good solution, the generated goals should focus on one alternative and hence the GAN should have a unimodal distribution. How do you force the GAN in a principled way to focus on one goal in this case? How could you combine RL and training the GAN stably in that case? Detailed: - (2) is a bit strange: shouldnt the indicator say: 1( exists t: s_t in S^g )? Surely not all states in the rollout (s_0 ... s_t) are in the goal subspace: the indicator does not factorize over the union. Same for other formulas that use union. - Are goals overlapping or non-overlapping subsets of the state space? Definition around (1) basically says its non-overlapping, yet the goal GAN seems to predict goals in a 2d space, hence the predicted goals are overlapping? - What are the goals that the non-uniform baselines predict? Does the GAN produce better goals? - Generating goal labels is - Paper should discuss literature on hierarchical methods that use goals learned from data and via variational methods: 1. Strategic Attentive Writer (STRAW), V. Mnih et al, NIPS 2016 2. Generating Long-term Trajectories Using Deep Hierarchical Networks. S. Zheng et al, NIPS 2016"	2	4	0.0451127819548872	0.9734	2018
1	"AnonReviewers 2 and AnonReviewer 3 rated the paper highly, with AR3 even upgrading their score. AnonReviewer1 was less generous: "" Overall, it is a good empirical study, raising a healthy set of questions. In this regard, the paper is worth accepting. However, I am still uncomfortable with the lack of answers and given that the revision does not include the additional discussion and experiments promised in the rebuttal, I will stay with my evaluation."" The authors have promised to produce the discussion and new experiments. Given the nature of both (1: the discussion is already outline in the response and 2: the experiments are straightforward to run), Im inclined to accept the paper because it represents a solid body of empirical work."	"The submission describes an empirical study regarding the training performance of GANs; more specifically, it aims to present empirical evidence that the theory of divergence minimization is more a tool to understand the outcome of training (i.e. Nash equillibrium) than a necessary condition to be enforce during training itself. The work focuses on studying ""non-saturating"" GANs, using the modified generator objective function proposed by Goodfellow et al. in their seminal GAN paper, and aims to show increased capabilities of this variant, compared to the ""standard"" minimax formulation. Since most theory around divergence minimization is based on the unmodified loss function for generator G, the experiments carried out in the submission might yield somewhat surprising results compared the theory. If I may summarize the key takeaways from Sections 5.4 and 6, they are: - GAN training remains difficult and good results are not guaranteed (2nd bullet point); - Gradient penalties work in all settings, but why is not completely clear; - NS-GANs + GPs seems to be best sample-generating combination, and faster than WGAN-GP. - Some of the used metrics can detect mode collapse. The submissions (counter-)claims are served by example (cf. Figure 2, or Figure 3 description, last sentence), and mostly relate to statements made in the WGAN paper (Arjovsky et al., 2017). As a purely empirical study, it poses more new and open questions on GAN optimization than it is able to answer; providing theoretical answers is deferred to future studies. This is not necessarily a bad thing, since the extensive experiments (both ""toy"" and ""real"") are well-designed, convincing and comprehensible. Novel combinations of GAN formulations (non-saturating with gradient penalties) are evaluated to disentangle the effects of formulation changes. Overall, this work is providing useful experimental insights, clearly motivating further study. "	4	5	0.030716723549488	0.9783	Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches. The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments. Clarity: The paper is well-written and clear. The authors could be more concise when reporting results. I would suggest keeping the main results in the main body and move extended results to an appendix. Originality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs. More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails. They also show experimentally that penalized gradients stabilize the learning process. Significance: The problems the authors consider is worth exploring further. The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally. However, publishing this work is in my opinion premature for the following reasons: - The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap; - The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory; - The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them. 	2	4	0.013215859030837	0.7876	"This paper answers recent critiques about ``standard GAN that were recently formulated to motivate variants based on other losses, in particular using ideas from optimal transport. It makes main points 1) ``standard GAN is an ill-defined term that may refer to two different learning criteria, with different properties 2) though the non-saturating variant (see Eq. 3) of ``standard GAN may converge towards a minimum of the Jensen-Shannon divergence, it does not mean that the minimization process follows gradients of the Jensen-Shannon divergence (and conversely, following gradient paths of the Jensen-Shannon divergence may not converge towards a minimum, but this was rather the point of the previous critiques about ``standard GAN). 3) the penalization strategies introduced for ``non-standard GAN with specific motivations, may also apply successfully to the ``standard GAN, improving robustness, thereby helping to set hyperparameters. Note that item 2) is relevant in many other setups in the deep learning framework and is often overlooked. Overall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered. In particular, the writing is sometimes ambiguous (e.g. in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross-references between authors, acronyms and formulae). The answers to the critiques referenced in the paper are convincing, though I must admit that I dont know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience. Details: - p. 4 please do not qualify KL as a distance metric - Section 4.3: ""Every GAN variant was trained for 200000 iterations, and 5 discriminator updates were done for each generator update"" is ambiguous: what is exactly meant by ""iteration"" (and sometimes step elsewhere)? - Section 4.3: the performance measure is not relevant regarding distributions. The l2 distance is somewhat OK for means, but it makes little sense for covariance matrices. "	3	3	0.0486322188449848	0.9587	2018
0	A version of GCNs of Kipf and Welling is introduced with (1) no non-linearity; (2) a basic form of (softmax) attention over neighbors where the attention scores are computed as the cosine of endpoints representations (scaled with a single learned scalar). There is a moderate improvement on Citeseer, Cora, Pubmed. Since the use of gates with GCNs / Graph neural networks is becoming increasingly common (starting perhaps with GGSNNs of Li et al, ICLR 2016)) and using attention in graph neural networks is also not new (see reviews and comments for references), the novelty is very limited. In order to make the submission more convincing the authors could: (1) present results on harder datasets; (2) carefully evaluate against other forms of attention (i.e. previous work). As it stands, though it is interesting to see that such simple model performs well on the three datasets, I do not see it as an ICLR paper. Pros: -- a simple model, achieves results close / on par with state of the art Cons: -- limited originality -- either results on harder datasets or / and evaluation agains other forms of attention (i.e. previous work) are needed 	The paper proposes a semi supervised learning algorithm for graph node classification. The Algorithm is inspired from Graph Neural Networks and more precisely graph convolutional NNs recently proposed by ref (Kipf et al 2016)) in the paper. These NNs alternate 2 types of layers: non linear projection and diffusion, the latter incorporates the graph relational information by constraining neighbor nodes to have close representations according to some “graph metrics”. The authors propose a model with simplified projection layers and more sophisticated diffusion ones, incorporating a simple attention mechanism. Experiments are performed on citation textual datasets. Comparisons with published results on the same datasets are presented. The paper is clear and develops interesting ideas relevant to semi-supervised graph node classification. One finding is that simple models perform as well as more complex ones in this setting where labeled data is scarce. Another one is the importance of integrating relational information for classifying nodes when it is available. The attention mechanism itself is extremely simple, and learns one parameter per diffusion layers. One parameter weights correlations between node embeddings in a diffusion layer. I understand that you tried more complex attention mechanisms, but the one finally selected is barely an attention mechanism and rather a simple “importance” weight. This is not a criticism, but this makes the title somewhat misleading. The experiments show that the proposed model is state of the art for graph node classification. The performance is on par with some other recent models according to table 2. The other tests are also interesting, but the comparison could have been extended to other models e.g. GCN. You advocate the role of the diffusion layers, and in the experiments you stack 3 to 4 such layers. It would be interesting to have indications on the compromise performance/ number of diffusion layers and on the evolution of these performances when adding such layers. The bibliography on semi-supervised learning in graphs for classification is light and should be enhanced. Overall this is an interesting paper with nice findings. The originality is however relatively limited in a field where many recent papers have been proposed, and the experiments need to be completed. 	3	4	0.0083565459610027	0.968	SUMMARY. The paper presents an extension of graph convolutional networks. Graph convolutional networks are able to model nodes in a graph taking into consideration the structure of the graph. The authors propose two extensions of GCNs, they first remove intermediate non-linearities from the GCN computation, and then they add an attention mechanism in the aggregation layer, in order to weight the contribution of neighboring nodes in the creation of the new node representation. Interestingly, the proposed linear model obtains results that are on-par with the state-of-the-art model, and the linear model with attention outperforms the state-of-the-art models on several standard benchmarks. ---------- OVERALL JUDGMENT The paper is, for the most part, clear, although some improvement on the presentation would be good (see below). An important issue the authors should address is the notation consistency, the indexes i and j are used for defining nodes and labels, please use another index for labels. It is very interesting that stripping standard GCN out of nonlinearities gives pretty much the same results, I would appreciate if the authors could give some insights of why this is the case. It seems to me that an important experiment is missing here, have the authors tried to apply the attention model with the standard GCN? I like the idea of using a very minimal attention mechanism. The similarity function used for the attention (cosine) is symmetric, this means that if two nodes are connected in both directions, they will be equally important for each other. But intuitively this is not true in general. It would be interesting if the authors could elaborate a bit more on the choice of the similarity function. ---------- DETAILED COMMENTS Page 2. I do not understand the point of so many details on Graph Laplacian Regularization. Page 2. The use of the term skip-grams is somewhat odd, it is not clear what the authors mean with that. Page 3. the natural random walk ??? Bottom of page 4. When the authors introduce the attention based network also introduce the input/embedding layer, I believe there is a better place to do so instead of that together with the most important contribution of the paper. 	3	3	0.0275482093663911	0.9884	The paper proposes graph-based neural network in which weights from neighboring nodes are adaptively determined. The paper shows importance of propagation layer while showing the non-linear layer does not have significant effect. Further the proposed method also provides class relation based on the edge-wise relevance. The paper is easy to follow and the idea would be reasonable. Importance of the propagation layer than the non-linear layer is interesting, and I think it is worth showing. Variance of results of AGNN is comparable or even smaller than GLN. This is a bit surprising because AGNN would be more complicated computation than GLN. Is there any good explanation of this low variance of AGNN? Interpretation of Figure 2 is not clear. All colored nodes except for the thick circle are labeled node? I couldnt judge those predictions are appropriate or not.	3	2	0.0287769784172661	0.9203	2018
